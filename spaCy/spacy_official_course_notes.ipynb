{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Official Training Course from spaCy - Notes\n",
    "\n",
    "- [Advanced NLP with spaCy](https://course.spacy.io/en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 - Finding words, phrases, names and concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence with ten words & 2 punctuation!\n",
      "\n",
      "这是一句十四个字&2个标点的话！\n"
     ]
    }
   ],
   "source": [
    "# `language` & `doc`\n",
    "# `language` is the class to create an object usually named \"nlp\"\n",
    "# When a `nlp` object was called on a string, spaCy tokenize the text and create a `doc` object\n",
    "# A `doc` object contains a series of `token` objects, which can be indexed.\n",
    "\n",
    " \n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.zh import Chinese\n",
    "\n",
    "# create doc object and serialize the string\n",
    "nlp_en = English()\n",
    "doc_en = nlp_en(\"This is a sentence with ten words & 2 punctuation!\")\n",
    "print(doc_en.text)\n",
    "\n",
    "print()\n",
    "\n",
    "# create doc object and serialize the string\n",
    "nlp_zh = Chinese()\n",
    "doc_zh = nlp_zh(\"这是一句十四个字&2个标点的话！\")\n",
    "print(doc_zh.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is a\n",
      "\n",
      "这\n",
      "是一\n"
     ]
    }
   ],
   "source": [
    "# A `token` object contains a word, punctuations, etc.\n",
    "# A `span` is a collection of `token` objects and can be indexed as well.\n",
    "\n",
    "# return token by the index\n",
    "first_token_en = doc_en[0]\n",
    "print(first_token_en.text)\n",
    "\n",
    "# return span using index\n",
    "sec_thrd_token_en = doc_en[1:3]\n",
    "print(sec_thrd_token_en.text)\n",
    "\n",
    "print()\n",
    "\n",
    "# return token by the index\n",
    "first_token_zh = doc_zh[0]\n",
    "print(first_token_zh.text)\n",
    "\n",
    "# return span using index\n",
    "sec_thrd_token_zh = doc_zh[1:3]\n",
    "print(sec_thrd_token_zh.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuation\n",
      "!\n",
      "\n",
      "2\n",
      "个\n"
     ]
    }
   ],
   "source": [
    "# return and use the index of tokens (to find the next token)\n",
    "\n",
    "some_token_en = doc_en[9]\n",
    "print(some_token_en.text)\n",
    "next_token_en = doc_en[some_token_en.i + 1]\n",
    "print(next_token_en.text)\n",
    "\n",
    "print()\n",
    "\n",
    "some_token_zh = doc_zh[9]\n",
    "print(some_token_zh.text)\n",
    "next_token_zh = doc_zh[some_token_zh.i + 1]\n",
    "print(next_token_zh.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with True\n",
      "& True\n",
      "2 True\n",
      "ten True\n",
      "\n",
      "句 True\n",
      "& True\n",
      "2 True\n",
      "十 True\n"
     ]
    }
   ],
   "source": [
    "# return index, and lexical attributes of a token\n",
    "word_token_en = doc_en[4]\n",
    "print(word_token_en.text, word_token_en.is_alpha)\n",
    "punc_token_en = doc_en[7]\n",
    "print(punc_token_en.text, punc_token_en.is_punct)\n",
    "num_token_en = doc_en[8]\n",
    "print(num_token_en.text, num_token_en.like_num)\n",
    "tnum_token_en = doc_en[5]\n",
    "print(tnum_token_en.text, tnum_token_en.like_num)\n",
    "\n",
    "print()\n",
    "\n",
    "# return index, and lexical attributes of a token\n",
    "word_token_zh = doc_zh[3]\n",
    "print(word_token_zh.text, word_token_zh.is_alpha)\n",
    "punc_token_zh = doc_zh[8]\n",
    "print(punc_token_zh.text, punc_token_zh.is_punct)\n",
    "num_token_zh = doc_zh[9]\n",
    "print(num_token_zh.text, num_token_zh.like_num)\n",
    "tnum_token_zh = doc_zh[4]\n",
    "print(tnum_token_zh.text, tnum_token_zh.like_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained statistical models\n",
    "import spacy\n",
    "\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp_zh = spacy.load('zh_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN\n",
      "is AUX\n",
      "looking VERB\n",
      "to PART\n",
      "hire VERB\n",
      "a DET\n",
      "lawyer NOUN\n",
      "from ADP\n",
      "Ireland PROPN\n",
      "for ADP\n",
      "1 NUM\n",
      "million NUM\n",
      "dollars NOUN\n",
      ". PUNCT\n",
      "\n",
      "四川 PROPN\n",
      "大学 NOUN\n",
      "的 PART\n",
      "董佳明 PROPN\n",
      "教授 NOUN\n",
      "在 ADP\n",
      "峨眉山 PROPN\n",
      "创办 VERB\n",
      "了 PART\n",
      "青年 NOUN\n",
      "科幻 ADJ\n",
      "文学 NOUN\n",
      "论坛 NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# print part of speech using pre-trained model\n",
    "doc_en = nlp_en(\"Apple is looking to hire a lawyer from Ireland for 1 million dollars.\")\n",
    "for token in doc_en:\n",
    "    print(token.text, token.pos_)\n",
    "\n",
    "print()\n",
    "\n",
    "doc_zh = nlp_zh(\"四川大学的董佳明教授在峨眉山创办了青年科幻文学论坛.\")\n",
    "for token in doc_zh:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj looking\n",
      "is AUX aux looking\n",
      "looking VERB ROOT looking\n",
      "to PART aux hire\n",
      "hire VERB xcomp looking\n",
      "a DET det lawyer\n",
      "lawyer NOUN dobj hire\n",
      "from ADP prep lawyer\n",
      "Ireland PROPN pobj from\n",
      "for ADP prep hire\n",
      "1 NUM compound million\n",
      "million NUM nummod dollars\n",
      "dollars NOUN pobj for\n",
      ". PUNCT punct looking\n",
      "\n",
      "四川 PROPN compound:nn 大学\n",
      "大学 NOUN nmod:assmod 教授\n",
      "的 PART case 大学\n",
      "董佳明 PROPN compound:nn 教授\n",
      "教授 NOUN nsubj 创办\n",
      "在 ADP case 峨眉山\n",
      "峨眉山 PROPN nmod:prep 创办\n",
      "创办 VERB ROOT 创办\n",
      "了 PART aux:asp 创办\n",
      "青年 NOUN compound:nn 论坛\n",
      "科幻 ADJ amod 论坛\n",
      "文学 NOUN compound:nn 论坛\n",
      "论坛 NOUN dobj 创办\n",
      ". PUNCT punct 创办\n"
     ]
    }
   ],
   "source": [
    "# syntactic dependencies\n",
    "for token in doc_en:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "\n",
    "print()\n",
    "\n",
    "for token in doc_zh:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Ireland GPE\n",
      "1 million dollars MONEY\n",
      "\n",
      "四川大学 ORG\n",
      "董佳明 PERSON\n"
     ]
    }
   ],
   "source": [
    "# named entities\n",
    "\n",
    "for ent in doc_en.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "print()\n",
    "\n",
    "for ent in doc_zh.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "determiner\n",
      "particle\n"
     ]
    }
   ],
   "source": [
    "# Explain terminologies spacy.explain()\n",
    "print(spacy.explain('det'))\n",
    "print(spacy.explain('PART'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 6\n",
      "[(13073357917303605362, 24, 26), (13073357917303605362, 29, 31), (13073357917303605362, 38, 40), (13073357917303605362, 53, 55), (13073357917303605362, 73, 75), (13073357917303605362, 104, 106)]\n",
      "Match found: 13073357917303605362 iOS 7\n",
      "Match found: 13073357917303605362 iOS 11\n",
      "Match found: 13073357917303605362 iOS 10\n",
      "Match found: 13073357917303605362 downloaded Fortnite\n",
      "Match found: 13073357917303605362 downloading Minecraft\n",
      "Match found: 13073357917303605362 download Winzip\n"
     ]
    }
   ],
   "source": [
    "# pattern matching\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"After making the iOS update you won't notice a radical system-wide \"\n",
    "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
    "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
    "    \"some tweaks once you delve a little deeper.\"\n",
    "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
    "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
    "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
    "    \"I also need to download Winzip?\"\n",
    ")\n",
    "\n",
    "pattern = [ [{'TEXT': 'iOS'}, {'IS_DIGIT': True}], [{'LEMMA': 'download'}, {\"POS\":'PROPN'}] ] \n",
    "\n",
    "matcher.add('IOS_DOWNLOAD_PATTERN', pattern)\n",
    "\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "print(matches)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", match_id, doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 - Large-scale data analysis with spaCy\n",
    "\n",
    "## vocab & string store\n",
    "\n",
    "### Data Structure\n",
    "\n",
    "A `doc` object contains `token` objects.\n",
    "\n",
    "A `vocab` object (nlp.vocab) contains `lexeme` objects.\n",
    "\n",
    "A `StringStore` object (`nlp.vocab.strings`) contains a lookup table between hashes (`lexeme`s) and corresponding strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5439657043933447811\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "# string to hashes\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "print(cat_hash)\n",
    "\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of lexeme\n",
    "- `lexeme` objects contains **context-independent** information about a word like `orth`(hash), `text`, `is_alpha`\n",
    "- `lexeme` refer to the exact word, not the root of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14692702688101715474\n",
      "have\n",
      "1248239241591158246\n",
      "has\n",
      "12960022596163002503\n",
      "had\n",
      "\n",
      "had 12960022596163002503 True\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"I have has had a cat\")\n",
    "\n",
    "for word in ['have', 'has', 'had']:\n",
    "    word_hash = nlp.vocab.strings[word]\n",
    "    print(word_hash)\n",
    "    print(nlp.vocab.strings[word_hash])\n",
    "\n",
    "print()\n",
    "\n",
    "lexeme = nlp.vocab['had']\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if there was no doc created?\n",
    "\n",
    "- Hashes can always be returned, but cannot be reversed to strings if no actual text was processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5439657043933447811\n",
      "4630374899898420537\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"[E018] Can't retrieve string for hash '5439657043933447811'. This usually refers to an issue with the `Vocab` or `StringStore`.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-431f86265ecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzh_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat_hash\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/py_env/spacy/lib/python3.6/site-packages/spacy/strings.pyx\u001b[0m in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '5439657043933447811'. This usually refers to an issue with the `Vocab` or `StringStore`.\""
     ]
    }
   ],
   "source": [
    "# hash can always be returned, but cannot be reversed to a string if no actual text was processed\n",
    "nlp = English()\n",
    "\n",
    "cat_hash = nlp.vocab.strings['cat']\n",
    "print(cat_hash)\n",
    "\n",
    "zh_hash = nlp.vocab.strings['好']\n",
    "print(zh_hash)\n",
    "\n",
    "print(nlp.vocab.strings[cat_hash])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc, token & span\n",
    "\n",
    "### `doc` object constructor\n",
    "\n",
    "- 3 arguemnts: `nlp.vocab`, words, spaces\n",
    "\n",
    "### `span` object constuctor\n",
    "\n",
    "- 3 arguments: `doc`, start_index, end_index+1\n",
    "- optional arguments: label (to name a span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "Hello world GREETING\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# words and spaces to create a doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False] # whether the word is followed by a space\n",
    "\n",
    "# create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# create a span manually\n",
    "span = Span(doc, 0, 2, label=\"GREETING\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span]\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best practices\n",
    "- convert the data to strings as late as possible to fully take advantage of `doc` and `span` data structure\n",
    "- use toekn attributes if possible, for example `token.i` for token index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Berlin looks like a nice city!\")\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        if doc[token.i+1].pos_ == \"VERB\":\n",
    "            print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors & Similarities\n",
    "\n",
    "- `doc`, `span`, and `token` all have `similarity` method to generate a similarity score between 0 and 1 comparing with another object\n",
    "- The default similarity score is cosine between word vectors\n",
    "- `doc` and `span` use the average of token vetors by default.\n",
    "- The score was generated using an algorithm like word2vec and a lot of text.\n",
    "- It requires `md` or `lg` model or above\n",
    "- Short phrases are better than long documents with a lot of irrelevant words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors are generated by pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.2009e-01 -3.0322e-02 -7.9859e-02 -4.6279e-01 -3.8600e-01  3.6962e-01\n",
      " -7.7178e-01 -1.1529e-01  3.3601e-02  5.6573e-01 -2.4001e-01  4.1833e-01\n",
      "  1.5049e-01  3.5621e-01 -2.1508e-01 -4.2743e-01  8.1400e-02  3.3916e-01\n",
      "  2.1637e-01  1.4792e-01  4.5811e-01  2.0966e-01 -3.5706e-01  2.3800e-01\n",
      "  2.7971e-02 -8.4538e-01  4.1917e-01 -3.9181e-01  4.0434e-04 -1.0662e+00\n",
      "  1.4591e-01  1.4643e-03  5.1277e-01  2.6072e-01  8.3785e-02  3.0340e-01\n",
      "  1.8579e-01  5.9999e-02 -4.0270e-01  5.0888e-01 -1.1358e-01 -2.8854e-01\n",
      " -2.7068e-01  1.1017e-02 -2.2217e-01  6.9076e-01  3.6459e-02  3.0394e-01\n",
      "  5.6989e-02  2.2733e-01 -9.9473e-02  1.5165e-01  1.3540e-01 -2.4965e-01\n",
      "  9.8078e-01 -8.0492e-01  1.9326e-01  3.1128e-01  5.5390e-02 -4.2423e-01\n",
      " -1.4082e-02  1.2708e-01  1.8868e-01  5.9777e-02 -2.2215e-01 -8.3950e-01\n",
      "  9.1987e-02  1.0180e-01 -3.1299e-01  5.5083e-01 -3.0717e-01  4.4201e-01\n",
      "  1.2666e-01  3.7643e-01  3.2333e-01  9.5673e-02  2.5083e-01 -6.4049e-02\n",
      "  4.2143e-01 -1.9375e-01  3.8026e-01  7.0883e-03 -2.0371e-01  1.5402e-01\n",
      " -3.7877e-03 -2.9396e-01  9.6518e-01  2.0068e-01 -5.6572e-01 -2.2581e-01\n",
      "  3.2251e-01 -3.4634e-01  2.7064e-01 -2.0687e-01 -4.7229e-01  3.1704e-01\n",
      " -3.4665e-01 -2.5188e-01 -1.1201e-01 -3.3937e-01  3.1518e-01 -3.2221e-01\n",
      " -2.4530e-01 -7.1571e-02 -4.3971e-01 -1.2070e+00  3.3365e-01 -5.8208e-02\n",
      "  8.0899e-01  4.2335e-01  3.8678e-01 -6.0797e-01 -7.3760e-01 -2.0547e-01\n",
      " -1.7499e-01 -3.7842e-03  2.1930e-01 -5.2486e-02  3.4869e-01  4.3852e-01\n",
      " -3.4471e-01  2.8910e-01  7.2554e-02 -4.8625e-01 -3.8390e-01 -4.4760e-01\n",
      "  4.3278e-01 -2.7128e-03 -9.0067e-01 -3.0819e-02 -3.8630e-01 -8.0798e-02\n",
      " -1.6243e-01  2.8830e-01 -2.6349e-01  1.7628e-01  3.5958e-01  5.7672e-01\n",
      " -5.4624e-01  3.8555e-02 -2.0182e+00  3.2916e-01  3.4672e-01  1.5398e-01\n",
      " -4.3446e-01 -4.1428e-02 -6.9588e-02  5.1513e-01 -1.3489e-01 -5.7239e-02\n",
      "  4.9241e-01  1.8643e-01  3.8596e-01 -3.7329e-02 -5.4216e-01 -1.8152e-01\n",
      "  4.3110e-01 -4.6967e-01  6.6801e-02  5.0323e-01 -2.4059e-01  3.6742e-01\n",
      "  2.9300e-01 -8.7883e-02 -4.7940e-01 -4.3431e-02 -2.6137e-01 -6.2658e-01\n",
      "  1.1446e-01  2.7682e-01  3.4800e-01  5.0018e-01  1.4269e-01 -3.3545e-01\n",
      " -3.9712e-01 -3.3121e-01 -3.4434e-01 -4.1627e-01 -3.5707e-03 -6.2350e-01\n",
      "  3.7794e-01 -1.6765e-01 -4.1954e-01 -3.3134e-01  3.1232e-01 -3.9494e-01\n",
      " -4.6921e-03 -4.8884e-01 -2.2059e-02 -2.6174e-01  1.7937e-01  3.6628e-01\n",
      "  5.8971e-02 -3.5991e-01 -4.4393e-01 -1.1890e-01  3.3487e-01  3.6505e-02\n",
      " -3.2788e-01  3.3425e-01 -5.6361e-01 -1.1190e-01  5.3770e-01  2.0311e-01\n",
      "  1.5110e-01  1.0623e-02  3.3401e-01  4.6084e-01  5.6293e-01 -7.5432e-02\n",
      "  5.4813e-01  1.9395e-01 -2.6265e-01 -3.1699e-01 -8.1778e-01  5.8169e-02\n",
      " -5.7866e-02 -1.1781e-01 -5.8742e-02 -1.4092e-01 -9.9394e-01 -9.4532e-02\n",
      "  2.3503e-01 -4.9027e-01  8.5832e-01  1.1540e-01 -1.5049e-01  1.9065e-01\n",
      " -2.6705e-01  2.5326e-01 -6.7579e-01 -1.0633e-02 -5.5158e-02 -3.1004e-01\n",
      " -5.8036e-02 -1.7200e-01  1.3298e-01 -3.2899e-01 -7.5481e-02  2.9425e-02\n",
      " -3.2949e-01 -1.8691e-01 -9.5323e-01 -3.5468e-01 -3.3162e-01  5.6441e-02\n",
      "  2.1790e-02  1.7182e-01 -4.4267e-01  6.9765e-01 -2.6876e-01  1.1659e-01\n",
      " -1.6584e-01  3.8296e-01  2.9109e-01  3.6318e-01  3.6961e-01  1.6305e-01\n",
      "  1.8152e-01  2.2453e-01  3.9866e-02 -3.7607e-02 -3.6089e-01  7.0818e-02\n",
      " -2.1509e-01  3.6551e-01 -5.1603e-01 -5.8102e-03 -4.8320e-01 -2.5068e-01\n",
      " -5.2062e-02 -2.0828e-01  2.9060e-01  2.2084e-02 -6.8123e-01  4.2063e-01\n",
      "  9.5973e-02  8.1720e-01 -1.5241e-01  6.2994e-01  2.6449e-01 -1.3516e-01\n",
      "  3.2450e-01  3.0503e-01  1.2357e-01  1.5107e-01  2.8327e-01 -3.3838e-01\n",
      "  4.6106e-02 -1.2361e-01  1.4516e-01 -2.7947e-02  2.6231e-02 -5.9591e-01\n",
      " -4.4183e-01  7.8440e-01 -3.4375e-02 -1.3928e+00  3.5248e-01  6.5220e-01]\n",
      "\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "print(doc[1].vector)\n",
    "print()\n",
    "print(len(doc[1].vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8852706036899473\n",
      "hot summer sunny\n",
      "0.5955382\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "doc1 = nlp(\"It's a hot summer day.\")\n",
    "doc2 = nlp(\"It's sunny outside.\")\n",
    "\n",
    "sim = doc1.similarity(doc2)\n",
    "print(sim)\n",
    "\n",
    "span1 = doc1[3:5]\n",
    "token1 = doc2[2]\n",
    "\n",
    "print(span1, token1)\n",
    "\n",
    "sim = span1.similarity(token1)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching - Combining Models and Rules\n",
    "\n",
    "- The difference between `Matcher` and `PhraseMatcher` is the input formats:\n",
    "    - Inputs of `Matcher` is a list of token patterns\n",
    "    - Inputs of `PhraseMatcher` is a `doc` object, e.g. `nlp(\"some text\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czech Republic\n",
      "Slovakia\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "with open(\"data/countries.json\", encoding='utf8') as f:\n",
    "    COUNTRIES = json.load(f)\n",
    "# print(COUNTRIES)\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Initialize PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# shorter version of [nlp(country) for country in COUNTRIES]\n",
    "pattern = list(nlp.pipe(COUNTRIES))\n",
    "\n",
    "# Add pattern to matcher\n",
    "matcher.add(\"COUNTRY\", pattern)\n",
    "\n",
    "# call the matcher on the doc and print the result\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in --> Namibia\n",
      "in --> South Africa\n",
      "Africa --> Cambodia\n",
      "of --> Kuwait\n",
      "as --> Somalia\n",
      "Somalia --> Haiti\n",
      "Haiti --> Mozambique\n",
      "in --> Somalia\n",
      "for --> Rwanda\n",
      "Britain --> Singapore\n",
      "War --> Sierra Leone\n",
      "of --> Afghanistan\n",
      "invaded --> Iraq\n",
      "in --> Sudan\n",
      "of --> Congo\n",
      "earthquake --> Haiti\n",
      "\n",
      "[('Namibia', 'GPE'), ('South Africa', 'GPE'), ('Cambodia', 'GPE'), ('Kuwait', 'GPE'), ('Somalia', 'GPE'), ('Haiti', 'GPE'), ('Mozambique', 'GPE'), ('Somalia', 'GPE'), ('Rwanda', 'GPE'), ('Singapore', 'GPE'), ('Sierra Leone', 'GPE'), ('Afghanistan', 'GPE'), ('Iraq', 'GPE'), ('Sudan', 'GPE'), ('Congo', 'GPE'), ('Haiti', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "with open(\"data/countries.json\") as f:\n",
    "    COUNTIRES = json.load(f)\n",
    "\n",
    "with open(\"data/country_text.txt\") as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# create a matcher and add patterns\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTIRES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "# create a doc and reset existing entities\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # create a span for each match with label \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    print(span.root.head.text, \"-->\", span.text)\n",
    "\n",
    "print()\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_==\"GPE\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Processing Pipelines\n",
    "\n",
    "## `nlp` pipeline\n",
    "\n",
    "- `text` --> `nlp` (`tokenizer`(always necessary and in the first place) -> `tagger` -> `parser` -> `ner` -> ...) --> `doc`\n",
    "\n",
    "## Components\n",
    "| Name | Description | Creates |\n",
    "| :--- | :--- | :--- |\n",
    "| tagger | Part-of-speech tagger | Token.tag, Token.pos |\n",
    "| parser | Dependency parser | Token.dep, Token.head, Doc.sents, Doc.noun_chunks |\n",
    "| ner | Named entity recognizer | Doc.ents, Token.ent_iob, Token.ent_type |\n",
    "| textcat | Text classifier | Doc.cats |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7fa30483c468>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7fa30483c4c0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7fa311880730>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7fa3118808d0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7fa30466fe08>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7fa3046d4308>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Pipeline Components\n",
    "\n",
    "- A custom component is a functions taking a `doc` as the input and returns a `doc`.\n",
    "- A custom component can be added to a pipeline using `nlp.add_pipe` method\n",
    "\n",
    "### Arguments for `nlp.add_pipe` method\n",
    "\n",
    "|Argument |\tDescription |\tExample |\n",
    "| :--- | :--- | :--- |\n",
    "|last |\tIf True, add last |\tnlp.add_pipe(component, last=True) |\n",
    "|first |\tIf True, add first |\tnlp.add_pipe(component, first=True) |\n",
    "|before |\tAdd before component |\tnlp.add_pipe(component, before=\"ner\") |\n",
    "|after |\tAdd after component |\tnlp.add_pipe(component, after=\"tagger\") |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length', 'tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "This document is 4 tokens long.\n"
     ]
    }
   ],
   "source": [
    "# # spacy 2 version\n",
    "\n",
    "# import spacy\n",
    "# def length_components(doc):\n",
    "#     doc_length = len(doc)\n",
    "#     print(f\"This document is {doc_length} tokens long.\")\n",
    "#     return doc\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# nlp.add_pipe(length_components, first=True)\n",
    "# print(nlp.pipe_names)\n",
    "\n",
    "# doc = nlp(\"This is a sentence\")\n",
    "\n",
    "\n",
    "# spacy 3 version\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component('length')\n",
    "def length_components(doc):\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('length', first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "doc = nlp(\"This is a sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal patterns:  [Golden Retriever, cat, turtle, Rattus norvegeicus]\n",
      "['tok2vec', 'tagger', 'parser', 'ner', 'ANIMAL', 'attribute_ruler', 'lemmatizer']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "# A more complex custom pipeline components\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "animals = ['Golden Retriever', 'cat', 'turtle', 'Rattus norvegeicus']\n",
    "animals_patterns = list(nlp.pipe(animals))\n",
    "print(\"Animal patterns: \", animals_patterns)\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", animals_patterns)\n",
    "\n",
    "@Language.component(\"ANIMAL\")\n",
    "def animal_component(doc):\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc,start,end,label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe('ANIMAL', after='ner')\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "doc = nlp(\"I have a cat and a Golden Retriever.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension Attributes\n",
    "- Add custom metadata to `doc`s, `token`s, and `span`s.\n",
    "- Accessible via the `._` property.\n",
    "- Registered on the global `Doc`, `Token`, `Span` using the `set_extension` method.\n",
    "\n",
    "### Extension Attributes Types\n",
    "- **Attribute Extension**: set a default value that can be overwritten.\n",
    "- **Property Extension**: \n",
    "    - Define a getter and an optional setting function. \n",
    "    - The getter function was only called when the attribute value is retrieved.\n",
    "    - `Span` extensions should almost always use a getter function. Because it is usually expensive or even impossible to manually set values to all `span`s.\n",
    "- **Method Extension**: \n",
    "    - Assign a function that becomes available as an object method.\n",
    "    - Let you pass arguments to the extension function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "# Attribute extension example\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register an attribute extension\n",
    "Token.set_extension(\"is_country\", default=False)\n",
    "\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "\n",
    "# Update value of a certain token\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llA\n",
      "snoitazilareneg\n",
      "era\n",
      "eslaf\n",
      ",\n",
      "gnidulcni\n",
      "siht\n",
      "eno\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Property extension example\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# define a getter function\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "# register the extension\n",
    "Token.set_extension(\"reversed\", getter=get_reversed)\n",
    "\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token._.reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Another property extension example on `Doc`\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# define a getter function\n",
    "def get_has_number(doc):\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "# register the property extension\n",
    "Doc.set_extension(\"has_number\", getter=get_has_number)\n",
    "\n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "\n",
    "print(doc._.has_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "# Method Extension Example on Span\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# define a getter function to_html\n",
    "def to_html(span, tag):\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "# register the extension\n",
    "Span.set_extension(\"to_html\", method=to_html)\n",
    "\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(tag=\"strong\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "# Another example of property extension on spans\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# define a getter function\n",
    "def get_wikipedia_url(span):\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "# Regisiter a extension\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    "    )\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent._.wikipedia_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Afghanistan', 'Åland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra']\n",
      "\n",
      "[('Afghanistan', 'Kabul'), ('Åland Islands', 'Mariehamn'), ('Albania', 'Tirana'), ('Algeria', 'Algiers'), ('American Samoa', 'Pago Pago'), ('Andorra', 'Andorra la Vella')]\n",
      "\n",
      "['countries']\n",
      "\n",
      "Czech Republic Prague\n",
      "Slovakia Bratislava\n"
     ]
    }
   ],
   "source": [
    "# Combining custom pipeline components with extension attributes\n",
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "\n",
    "with open(\"data/countries.json\", 'r') as f:\n",
    "    COUNTRIES = json.load(f)\n",
    "\n",
    "with open(\"data/capitals.json\", 'r') as f:\n",
    "    CAPITALS = json.load(f)\n",
    "\n",
    "print(COUNTRIES[:6])\n",
    "print()\n",
    "print(list(CAPITALS.items())[:6])\n",
    "print()\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# create a phrase matcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "# define component function\n",
    "@Language.component(\"countries\")\n",
    "def countries_component(doc):\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label='GPE') for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(\"countries\")\n",
    "print(nlp.pipe_names)\n",
    "print()\n",
    "\n",
    "# Getter to look up the span text in a dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension\n",
    "Span.set_extension('capital', getter=get_capital)\n",
    "\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent._.capital)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling & Performance\n",
    "\n",
    "- use `nlp.pipe` to process a large number of texts more efficiently\n",
    "    - Bad way: `[nlp(text) for text in text_list]`\n",
    "    - Good way: `list(nlp.pipe(text_list))`\n",
    "\n",
    "- passing in (text, context) tuple (use as_tuple=True argument)\n",
    "    - yield (`doc`, `context`) tuples\n",
    "    - Useful for associating metadata with the `doc`\n",
    "    - values in `context`s can be assigned to extensions later\n",
    "\n",
    "- Run only parts of the pipeline\n",
    "     - use only the tokenizer: `doc = nlp.make_doc(\"some text\")`\n",
    "     - disable pipeline components within a with block: `with nlp.disable_pipes(\"tagger\", \"parser\"):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "['open']\n",
      "['terrible', 'payin']\n"
     ]
    }
   ],
   "source": [
    "# Use nlp.pipe to process a stream of texts\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "with open('data/tweets.json', 'r') as f:\n",
    "    TEXTS = json.load(f)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "for doc in list(nlp.pipe(TEXTS)):\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.\n",
      " - 'Metamorphosis' by Franz Kafka\n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing.\n",
      " - 'Moby-Dick or, The Whale' by Herman Melville\n",
      "\n",
      "It was the best of times, it was the worst of times.\n",
      " - 'A Tale of Two Cities' by Charles Dickens\n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.\n",
      " - 'On the Road' by Jack Kerouac\n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen.\n",
      " - '1984' by George Orwell\n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing.\n",
      " - 'The Picture Of Dorian Gray' by Oscar Wilde\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# processing data with context\n",
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "with open('data/bookquotes.json','r') as f:\n",
    "    DATA = json.load(f)\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register extensions\n",
    "Doc.set_extension('author', default=None)\n",
    "Doc.set_extension('book', default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    doc._.author = context['author']\n",
    "    doc._.book = context['book']\n",
    "\n",
    "    print(f\"{doc.text}\\n - '{doc._.book}' by {doc._.author}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "# Only tokenize the text using nlp.make_doc()\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-04-05 17:38:38,253] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'Chick'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,253] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '-'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,254] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'fil'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,254] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '-'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,255] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'A'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,255] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'is'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,255] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'an'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,256] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'American'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,256] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'fast'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,257] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'food'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,257] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'restaurant'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,257] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'chain'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,258] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'headquartered'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,258] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'in'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,258] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'the'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,259] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'city'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,259] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'of'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,260] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'College'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,260] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'Park'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,260] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,261] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'Georgia'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,261] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token ','. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,261] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'specializing'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,262] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'in'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,262] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'chicken'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,263] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token 'sandwiches'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "[2021-04-05 17:38:38,263] [WARNING] [W108] The rule-based lemmatizer did not find POS annotation for the token '.'. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "# run parts of pipeline using nlp.disable_pipes()\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = (\n",
    "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
    "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    ")\n",
    "\n",
    "with nlp.disable_pipes(\"tagger\", \"parser\"):\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 - Training a neural network model\n",
    "\n",
    "- re-train the pre-trained model on new data to improve accuracy of the particular problem\n",
    "\n",
    "## Creating a dataset\n",
    "\n",
    "- add new dataset (e.g., observations with new categories)\n",
    "- `Matcher` or `PhraseMatcher` can be handy in creating patterns to match new labels\n",
    "- If new categories were added, make sure to include some existing categories as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: [(20, 28, 'GADGET')]\n",
      "training example: ('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET')]})\n",
      "\n",
      "entity: [(0, 8, 'GADGET')]\n",
      "training example: ('iPhone X is coming', {'entities': [(0, 8, 'GADGET')]})\n",
      "\n",
      "entity: [(28, 36, 'GADGET')]\n",
      "training example: ('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET')]})\n",
      "\n",
      "entity: [(4, 12, 'GADGET')]\n",
      "training example: ('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]})\n",
      "\n",
      "entity: [(0, 9, 'GADGET'), (13, 21, 'GADGET')]\n",
      "training example: (\"iPhone 11 vs iPhone 8: What's the difference?\", {'entities': [(0, 9, 'GADGET'), (13, 21, 'GADGET')]})\n",
      "\n",
      "entity: []\n",
      "training example: ('I need a new phone! Any tips?', {'entities': []})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a training dataset to update the model to recognize \"iphone ?\" as \"GADGET\"\n",
    "\n",
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "with open(\"data/iphone.json\",'r') as f:\n",
    "    TEXT = json.load(f)\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [[{'LOWER': 'iphone'}, {\"LOWER\": 'x'}], [{'LOWER': 'iphone'}, {\"IS_DIGIT\": True}]]\n",
    "matcher.add(\"GADGET\", pattern)\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "for doc in nlp.pipe(TEXT):\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
    "    print('entity:', entities)\n",
    "    training_example = (doc.text, {'entities': entities})\n",
    "    print('training example:', training_example)\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "1. **Loop** for a number of times.\n",
    "2. **Shuffle** the training data.\n",
    "3. **Divide** the data into mini-batches.\n",
    "4. **Update** the model for each batch.\n",
    "5. **Save** the updated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 25.834728956222534}\n",
      "{'ner': 18.855169266462326}\n",
      "{'ner': 8.088818974792957}\n",
      "{'ner': 4.0973786945978645}\n",
      "{'ner': 11.280892569835714}\n",
      "{'ner': 5.674000441357663}\n",
      "{'ner': 3.2181017984692755}\n",
      "{'ner': 1.3580093800089275}\n",
      "{'ner': 0.8035100147845582}\n",
      "{'ner': 0.6508244672133929}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "\n",
    "with open('data/gadgets.json', 'r') as f:\n",
    "    TRAINING_DATA = json.load(f)\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "ner = nlp.add_pipe('ner')\n",
    "ner.add_label(\"GADGET\")\n",
    "\n",
    "nlp.begin_training()\n",
    "\n",
    "for itn in range(10):\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        # texts = [text for text, entities in batch]\n",
    "        # annotations = [entities for text, entities in batch]\n",
    "\n",
    "        examples = [Example.from_dict(nlp.make_doc(text), annotation) for text, annotation in batch]\n",
    "        nlp.update(examples, losses=losses)\n",
    "\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('spacy': venv)",
   "language": "python",
   "name": "python36964bitspacyvenvdb66deeded124d3d96928232f0204a60"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "68cf78d9397dcc9b12a9f626df1265230568d8d9e85658a1f8db906be3e84089"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}